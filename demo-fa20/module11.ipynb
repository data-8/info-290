{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datascience import *\n",
    "from math import *\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', warn=False)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Review: Creating a confidence interval\n",
    "\n",
    "How accurate is the confidence interval process? Let's find out! We have some population-level (let's assume) data of city employee salaries in Oakland in 2018. (Source: Transparent California)\n",
    "\n",
    "In reality, you would not often have access to information of this level. Let's assume that we only have access to samples from this population, and see if the confidence interval process works!\n",
    "\n",
    "We're also going to do a hypothesis test (for practice; this probably isn't the best application) with a confidence interval here. In 2019, the average total compensation -- salary and benefits -- for a full-time employee in California cities was approximately $ 174,000 (California Globe). Is it the same for Oakland? \n",
    "\n",
    "H0: The average total pay and benefits for a city employee in Oakland is $174,000.\n",
    "\n",
    "Ha: The average total pay and benefits for a city employee in Oakland is not $174,000.\n",
    "\n",
    "Note: Does that number seem kind of high? In this analysis, we include both firefighters and police officers, who tend to have higher salaries and wages, and therefore skew the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oakland = Table().read_table(\"oakland-2018.csv\").where(\"Status\", \"FT\") #Full time workers only\n",
    "pop_mean = np.mean(oakland.column(\"Total Pay & Benefits\"))\n",
    "oakland.show(5)\n",
    "print(\"The population mean is: $\" + str(np.round(pop_mean, 2))) # For readability: don't round the mean in your own code!\n",
    "oakland.hist(\"Total Pay & Benefits\")\n",
    "plots.scatter(x = pop_mean, y = 0, color = \"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Let's write a function that does the following 3 things:\n",
    "# 1) take a random sample from TBL without replacement of sample size N\n",
    "# 2) bootstrap REPS times\n",
    "# 3) generate a confidence interval of C percent confidence for the mean \"Total Pay\"\n",
    "# 4) return the interval as a 2 item ARRAY (lower, upper)\n",
    "# You can assume TBL is the same structure as the oakland table above\n",
    "\n",
    "def one_full_cycle(tbl, n, reps, c):\n",
    "    one_sample = ...\n",
    "    ...\n",
    "    for ...:\n",
    "        ...\n",
    "    lower_bound = ...\n",
    "    upper_bound = ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell!\n",
    "one_interval = one_full_cycle(oakland, 50, 1000, 90)\n",
    "print(\"One random interval has a lower bound of \" + str(one_interval.item(0)) + \" and an upper bound of \" + str(one_interval.item(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: if we ran the code above 100 times, how many intervals would capture the pop mean on average?\n",
    "# Type your guess in estimated_correct below.\n",
    "num_intervals = 100\n",
    "estimated_correct = ...\n",
    "\n",
    "## Q3: What does the code below do? Explain to your peers (or type an explanation out yourself)\n",
    "# NOTE: Running the cell will take a decently long time, so be patient! (lot of computations happening)\n",
    "num_correct = 0\n",
    "for i in np.arange(num_intervals):\n",
    "    interval = one_full_cycle(oakland, 50, 1000, 90)\n",
    "    num_correct = num_correct + (interval.item(0) <= pop_mean <= interval.item(1)) \n",
    "    \n",
    "num_correct\n",
    "\n",
    "# Q3.2: if your guess was close but not exact, what could we do \n",
    "# to get a better estimate of the \"true\" number of correct intervals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Do we reject or fail to reject our null hypothesis?  Set reject_null to True if we reject\n",
    "# and False to if we fail to reject. \n",
    "# Use the interval, one_interval, from above, to make this conclusion. \n",
    "\n",
    "reject_null = ???\n",
    "\n",
    "#Q5: What is our significance level for this test? \n",
    "\n",
    "significance_level = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Center & Spread\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Out of scope of this class (but mentioned in the textbook!):\n",
    "# Unlike R, Python is not designed for statistical analysis\n",
    "# BUT - we can use libraries for statistical analysis, like numpy and scipy\n",
    "\n",
    "from scipy import stats # Import the stats module from scipy; call with stats.function(...)\n",
    "# Lot of cool stats functions! \n",
    "\n",
    "# For example: Mayor Schaaf made $314,400 in Total Pay & Benefits. What percentile of employees was she at?\n",
    "# Our function: percentileofscore(array, value)\n",
    "pay_benefits = oakland.column(\"Total Pay & Benefits\")\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measures of center in Python\n",
    "np.mean(...)\n",
    "np.median(...)\n",
    "percentile(..., ...)  # Note: percentile is a datascience library function; np.percentile(array, percent) is in numpy\n",
    "stats.mode(..., axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measures of spread in Python\n",
    "np.std(...)\n",
    "np.ptp(...) # range\n",
    "stats.iqr(...) # interquartile range: 75th quartile - 25th quartile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall: Variance = average of the squared differences from the mean\n",
    "# short-answer: use np.var\n",
    "print(\"Numpy says the variance is: \" + str(np.var(pay_benefits)))\n",
    "\n",
    "# Now, do it yourself: write a function that calculates the variance of an array\n",
    "def your_var(arr):\n",
    "    ...\n",
    "\n",
    "your_var(pay_benefits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that calculates the standard deviation!\n",
    "# Don't forget about abstraction\n",
    "\n",
    "print(\"Numpy says the standard deviation is: \" + str(np.std(pay_benefits)))\n",
    "\n",
    "def your_std(arr):\n",
    "   ...\n",
    "\n",
    "your_std(pay_benefits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality\n",
    "\n",
    "The normal curve is a particular bell-shaped distribution, where 68% of data is +-1 SD from the mean, 95% +- 2 SD from the mean, and 99% +- 3 SD from the mean. Contrast this with all distributions which follow Chebyshev's bounds: at least 0% for 1 SD, 75% for 2 SDs, and 88% for 3 SDs. Furthermore, the normal curve is nice in that the SD is the distance between the mean and the point of inflection on the curve on the left or right. \n",
    "\n",
    "Note that, because we are using a simulation to approximate some values for this exercise, we may NOT have the exact results we're looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Before we begin, we need to assume the Central Limit Theorem holds\n",
    "# in this case: we have a large random sample using the sample mean or sample sum\n",
    "# Let's look at the data!\n",
    "# We're going to use Total Pay in this specific case, NOT pay and benefits\n",
    "\n",
    "oakland.hist(\"Total Pay\", bins = np.arange(0, 400000, 50000)) ## The population distribution\n",
    "plots.title(\"Population\");\n",
    "\n",
    "our_sample = oakland.sample(30, with_replacement = False) # notice sample size of 30\n",
    "our_sample.hist(\"Total Pay\", bins = np.arange(0, 400000, 50000)) ## Our sample distribution\n",
    "plots.title(\"Sample\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: Now, as we've been doing, let's do a bootstrap and create a distribution of sample means\n",
    "# Remember that the goal of the bootstrap is to approximate the process of sampling from the population\n",
    "\n",
    "sample_means = make_array()\n",
    "trials = 1000\n",
    "\n",
    "for ...\n",
    "\n",
    "means_tbl = Table().with_column(\"Resampled Means\", sample_means)\n",
    "means_tbl.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looks good! Q: Is it approx. normal as stated by the CLT? Let's check using a \"back of the envelope\" test\n",
    "# i.e. check if the 68-95-99.7 rule works here\n",
    "\n",
    "sd = ...\n",
    "avg = ...\n",
    "\n",
    "\n",
    "# Hint: use tbl.where\n",
    "means_tbl..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One last note: what happens to the variation when we change the sample size?\n",
    "print(\"Our SD with a sample size of 30 was: \" + str(sd))\n",
    "\n",
    "new_sample_size = ...\n",
    "our_new_sample = oakland.sample(new_sample_size, with_replacement = False)\n",
    "new_sample_means = make_array()\n",
    "\n",
    "for i in np.arange(1000):\n",
    "    resample = our_new_sample.sample()\n",
    "    resamp_mean = np.mean(resample.column(\"Total Pay\"))\n",
    "    new_sample_means = np.append(new_sample_means, resamp_mean)\n",
    "    \n",
    "# What's our std now?\n",
    "print(\"Our SD with a sample size of \" + str(new_sample_size) + \" was: \" + str(np.std(new_sample_means)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why does this occur?\n",
    "\n",
    "# Conceptual explanation:\n",
    "\n",
    "# Mathematical explanation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation: What is the relationship between 2 variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does one's base salary correlate with the amount of benefits they receive?\n",
    "# Let's check graphically:\n",
    "\n",
    "oakland.scatter(\"Base Pay\", \"Benefits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It would appear so! But we need to prove it quantitatively.\n",
    "# First, notice the ranges of the axes.\n",
    "np.ptp(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A potential issue is any calculation we make\n",
    "# May be more reflective of the vast differences in the ranges,\n",
    "# rather than an actual correlation. So first, we need to standardize!\n",
    "\n",
    "def standard_units(arr):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_oak = ...\n",
    "\n",
    "standard_oak.scatter(0, 1)\n",
    "\n",
    "# What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now, let's quantify the relationship between the 2 variables\n",
    "# We will use the correlation coefficient, r\n",
    "\n",
    "def r(tbl):\n",
    "    \"\"\"Given a 2 column table of x and y in STANDARD UNITS, calculate the correlation coefficient\"\"\"\n",
    "   ...\n",
    "\n",
    "r(standard_oak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What should we conclude from above?\n",
    "\n",
    "# as another tip: another way we can calculate R (without all of this code):\n",
    "stats.pearsonr(oakland.column(\"Base Pay\"), oakland.column(\"Benefits\"))\n",
    "\n",
    "# SciPy reports 2 values: the correlation coefficient and the p-value (w/ the null that x & y are uncorrelated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
