{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Building Classification Models\n",
    "\n",
    "Welcome to the final project of Data 8X!  You will build two classifiers, looking at movie genres and code violations.  We recommend completing 8.3X Lab 14 before you work on this and watching the videos for Module 14. By the end of the project, you should know how to:\n",
    "\n",
    "1. Build a k-nearest-neighbors classifier.\n",
    "2. Test a classifier on data.\n",
    "\n",
    "**Please answer all questions, including the free response portions.**\n",
    "\n",
    "### Logistics\n",
    "\n",
    "**Deadline.** This project is due at 11:59pm on Friday, 12/18. Due to final grade deadlines, we cannot grant extensions for this project and we will not accept any late submissions. It's **much** better to be early than late, so start working now.\n",
    "\n",
    "**Support.** If you need help on the project, please come to office hours, post on Piazza, and/or talk to your classmates. If you need a guide, please reference the textbook here: https://www.inferentialthinking.com/chapters/17/Classification.html\n",
    "\n",
    "**Grading.** There are NO autograded tests for this assignment and you will be graded on correctness. Make sure your function has the correct output when examples are provided. If you have any questions about your solutions, please create a private post on Piazza with screenshots of your code. \n",
    "\n",
    "**Advice.** Develop your answers incrementally. To perform a complicated table manipulation, break it up into steps, perform each step on a different line, give a new name to each result, and check that each intermediate result is what you expect. You can add any additional names or functions you want to the provided cells. Also, please be sure to not re-assign variables throughout the notebook! For example, if you use max_temperature in your answer to one question, do not reassign it later on.\n",
    "\n",
    "To get started, load `datascience`, `numpy`, `plots`, and `otter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up the notebook, but please don't change it.\n",
    "import numpy as np\n",
    "import math\n",
    "from datascience import *\n",
    "\n",
    "# These lines set up the plotting functionality and formatting.\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', warn=False)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# These lines load the tests.\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. The Movies Dataset\n",
    "\n",
    "In this section of the project, we are exploring movie screenplays. We'll be trying to predict each movie's genre from the text of its screenplay. In particular, we have compiled a list of 5,000 words that might occur in the dialog of a movie. For each movie, our dataset tells us the frequency with which each of these words occurs in its screenplay. All words have been converted to lowercase.\n",
    "\n",
    "Run the cell below to read the `movies` table. **It may take up to a minute to load.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = Table.read_table('movies.csv')\n",
    "movies.where(\"Title\", \"the matrix\").select(0, 1, 2, 3, 4, 5, 10, 30, 5005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell prints a few columns of the row for the action movie *The Matrix*.  The movie contains 3792 words. The word \"it\" appears 115 times, as it makes up  $\\frac{115}{3792} \\approx 0.030327$ of the words in the movie. The word \"not\" appears 33 times, as it makes up $\\frac{33}{3792} \\approx 0.00870253$ of the words. The word \"fling\" doesn't appear at all.\n",
    "\n",
    "This numerical representation of a body of text, one that describes only the frequencies of individual words, is called a bag-of-words representation. A lot of information is discarded in this representation: the order of the words, the context of each word, who said what, the cast of characters and actors, etc. However, a bag-of-words representation is often used for machine learning applications as a reasonable starting point, because a great deal of information is also retained and expressed in a convenient and compact format. In this project, we will investigate whether this representation is sufficient to build an accurate genre classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All movie titles are unique. The `row_for_title` function provides fast access to the one row for each title. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = movies.index_by('Title')\n",
    "def row_for_title(title):\n",
    "    \"\"\"Return the row for a title, similar to the following expression (but faster)\n",
    "    \n",
    "    movies.where('Title', title).row(0)\n",
    "    \"\"\"\n",
    "    return title_index.get(title)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 1:** In the following cell, use the `row_for_title` function to find the proportion of times \"hey\" was stated in the movie, The Terminator. A row is a specific data type, so you can find the value of a specific label by using `.item(\"word\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset was extracted from [a Kaggle dataset from Cornell University](https://www.kaggle.com/Cornell-University/movie-dialog-corpus). After transforming the dataset (e.g., converting the words to lowercase, removing the naughty words, and converting the counts to frequencies), we created this new dataset containing the frequency of 5000 common words in each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Words with frequencies:', movies.drop(np.arange(6)).num_columns) \n",
    "print('Movies with genres:', movies.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Stemming\n",
    "\n",
    "How was the dataset generated? The columns other than \"Title\", \"Genre\", \"Year\", \"Rating\", \"# Votes\" and \"# Words\" in the `movies` table are all words that appear in some of the movies in our dataset.  These words have been *stemmed*, or abbreviated heuristically, in an attempt to make different [inflected](https://en.wikipedia.org/wiki/Inflection) forms of the same base word into the same string.  For example, the column \"manag\" is the sum of proportions of the words \"manage\", \"manager\", \"managed\", and \"managerial\" (and perhaps others) in each movie. This is a common technique used in machine learning and natural language processing.\n",
    "\n",
    "Stemming makes it a little tricky to search for the words you want to use, so we have provided another table that will let you see examples of unstemmed versions of each stemmed word.  Run the code below to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just run this cell.\n",
    "vocab_mapping = Table.read_table('stem.csv')\n",
    "stemmed = np.take(movies.labels, np.arange(3, len(movies.labels)))\n",
    "vocab_table = Table().with_column('Stem', stemmed).join('Stem', vocab_mapping)\n",
    "vocab_table.take(np.arange(1100, 1110))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Coding Question 2:** Assign `stemmed_message` to the stemmed version of the word \"alternating\".\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_1_1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_message = ...\n",
    "stemmed_message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "**Coding Question 3:** Assign `unstemmed_run` to an array of words in `vocab_table` that have \"run\" as its stemmed form. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_1_2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unstemmed_run = ...\n",
    "unstemmed_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis: Linear Regression\n",
    "\n",
    "Let's explore our dataset before trying to build a classifier. To start, we'll look at the relationship between words in proportions. \n",
    "\n",
    "The first association we'll investigate is the association between the proportion of words that are \"outer\" and the proportion of words that are \"space\". \n",
    "\n",
    "As usual, we'll investigate our data visually before performing any numerical analysis.\n",
    "\n",
    "Run the cell below to plot a scatter diagram of space proportions vs outer proportions and to create the `outer_space` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell!\n",
    "outer_space = movies.select(\"outer\", \"space\")\n",
    "outer_space.scatter(\"outer\", \"space\")\n",
    "plots.axis([-0.001, 0.0025, -0.001, 0.005]);\n",
    "plots.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 3:** Looking at that chart it is difficult to see if there is an association. Calculate the correlation coefficient for the association between proportion of words that are \"outer\" and the proportion of words that are \"space\" for every movie in the dataset, and assign it to `outer_space_r`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_2_1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our solution took multiple lines\n",
    "# these two arrays should make your code cleaner!\n",
    "outer = movies.column(\"outer\")\n",
    "space = movies.column(\"space\")\n",
    "\n",
    "outer_su = ...\n",
    "space_su = ...\n",
    "\n",
    "outer_space_r = ...\n",
    "outer_space_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 5:**\n",
    "Choose two *different* words in the dataset with a correlation higher than 0.2 or smaller than -0.2 that are not *outer* and *space* and plot a scatter plot with a line of best fit for them. The code to plot the scatter plot and line of best fit is given for you, you just need to calculate the correct values to `r`, `slope` and `intercept`.\n",
    "\n",
    "*Hint: It's easier to think of words with a positive correlation, i.e. words that are often mentioned together*.\n",
    "\n",
    "*Hint 2: Try to think of common phrases or idioms*.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_2_2\n",
    "manual: true\n",
    "image: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_x = ...\n",
    "word_y = ...\n",
    "\n",
    "# These arrays should make your code cleaner!\n",
    "arr_x = movies.column(word_x)\n",
    "arr_y = movies.column(word_y)\n",
    "\n",
    "x_su = ...\n",
    "y_su = ...\n",
    "\n",
    "r = ...\n",
    "\n",
    "slope = ...\n",
    "intercept = ...\n",
    "\n",
    "# DON'T CHANGE THESE LINES OF CODE\n",
    "movies.scatter(word_x, word_y)\n",
    "max_x = max(movies.column(word_x))\n",
    "plots.title(f\"Correlation: {r}, magnitude greater than .2: {abs(r) >= 0.2}\")\n",
    "plots.plot([0, max_x * 1.3], [intercept, intercept + slope * (max_x*1.3)], color='gold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the K-Nearest Neighbors Classifier\n",
    "\n",
    "K-Nearest Neighbors (k-NN) is a classification algorithm.  Given some numerical *attributes* (also called *features*) of an unseen example, it decides whether that example belongs to one or the other of two categories based on its similarity to previously seen examples. Predicting the category of an example is called *labeling*, and the predicted category is also called a *label*.\n",
    "\n",
    "An attribute (feature) we have about each movie is *the proportion of times a particular word appears in the movies*, and the labels are two movie genres: romance and action.  The algorithm requires many previously seen examples for which both the attributes and labels are known: that's the `train_movies` table. We will create that table below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Splitting the dataset\n",
    "We're going to use our `movies` dataset for two purposes.\n",
    "\n",
    "1. First, we want to *train* movie genre classifiers.\n",
    "2. Second, we want to *test* the performance of our classifiers.\n",
    "\n",
    "Hence, we need two different datasets: *training* and *test*.\n",
    "\n",
    "The purpose of a classifier is to classify unseen data that is similar to the training data. Therefore, we must ensure that there are no movies that appear in both sets. We do so by splitting the dataset randomly. The dataset has already been permuted randomly, so it's easy to split.  We just take the top for training and the rest for test. \n",
    "\n",
    "Run the code below to separate the datasets into two tables. Make sure you change `training_proportion` before you run the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split the dataset into two separate sets. \n",
    "## Choose the best proportion to generate your training set; the following code will split movies into 2 sets. \n",
    "# The test_proportion will be  1 - training_proportion. Your training proportion should be a majority \n",
    "# of the dataset. \n",
    "\n",
    "training_proportion = ...\n",
    "\n",
    "### Do not change the lines below. \n",
    "num_movies = movies.num_rows\n",
    "num_train = int(num_movies * training_proportion)\n",
    "num_valid = num_movies - num_train\n",
    "\n",
    "train_movies = movies.take(np.arange(num_train))\n",
    "test_movies = movies.take(np.arange(num_train, num_movies))\n",
    "\n",
    "print(\"Training: \",   train_movies.num_rows, \";\",\n",
    "      \"Test: \",       test_movies.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 1:** Why did you choose the following proportion? What would be the issue later on in classification if we set the training_proportion to 1? What is the goal of randomly permuting the dataset before splitting it? What is the goal of splitting the dataset into both a training and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 6:** Draw a horizontal bar chart with two bars that show the proportion of Action movies in each dataset.  Complete the function `action_proportion` first; it should help you create the bar chart.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1_2_1\n",
    "manual: true\n",
    "image: true\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def action_proportion(table):\n",
    "    # Return the proportion of movies in a table that have the Action genre.\n",
    "    return ...\n",
    "\n",
    "# The staff solution took multiple lines.  Start by creating a table.\n",
    "# If you get stuck, think about what sort of table you need for barh to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Choosing an Algorithm\n",
    "\n",
    "In k-NN, we classify a movie by finding the `k` movies in the *training set* that are most similar according to the features we choose. We call those movies with similar features the *nearest neighbors*.  The k-NN algorithm assigns the movie to the most common category among its `k` nearest neighbors.\n",
    "\n",
    "Let's limit ourselves to just 2 features for now, so we can plot each movie.  The features we will use are the proportions of the words \"water\" and \"feel\" in the movie.  Taking the movie *Monty Python and the Holy Grail* (in the test set), 0.000804074 of its words are \"water\" and 0.0010721 are \"feel\". This movie appears in the test set, so let's imagine that we don't yet know its genre.\n",
    "\n",
    "First, we need to make our notion of similarity more precise.  We will say that the *distance* between two movies is the straight-line distance between them when we plot their features in a scatter diagram. \n",
    "\n",
    "**This distance is called the Euclidean (\"yoo-KLID-ee-un\") distance, whose formula is $\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}$.**\n",
    "\n",
    "For example, in the movie *Clerks.*, 0.00016293 of all the words in the movie are \"water\" and 0.00154786 are \"feel\".  Its distance from *Monty Python and the Holy Grail* on this 2-word feature set is $\\sqrt{(0.000804074 - 0.000162933)^2 + (0.0010721 - 0.00154786)^2} \\approx 0.000798379$.  (If we included more or different features, the distance could be different.)\n",
    "\n",
    "A third movie, *The Avengers* (in the training set), is 0 \"water\" and 0.00103173 \"feel\".\n",
    "\n",
    "The function below creates a plot to display the \"water\" and \"feel\" features of a test movie and some training movies. As you can see in the result, *Monty Python and the Holy Grail* is more similar to \"Clerks.\" than to the *The Avengers* based on these features, which is makes sense as both movies are comedy movies, while *The Avengers* is a thriller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do not change the code in this cell.\n",
    "def plot_movie_with_two_features(test_movie, training_movies, x_feature, y_feature):\n",
    "    \"\"\"Plot a test movie and training movies using two features.\"\"\"\n",
    "    test_row = row_for_title(test_movie)\n",
    "    distances = Table().with_columns(\n",
    "            x_feature, [test_row.item(x_feature)],\n",
    "            y_feature, [test_row.item(y_feature)],\n",
    "            'Color',   ['unknown'],\n",
    "            'Title',   [test_movie]\n",
    "        )\n",
    "    for movie in training:\n",
    "        row = row_for_title(movie)\n",
    "        distances.append([row.item(x_feature), row.item(y_feature), row.item('Genre'), movie])\n",
    "    distances.scatter(x_feature, y_feature, colors='Color', labels = \"Title\", s=30)\n",
    "    \n",
    "\n",
    "training = [\"the terminator\", \"the avengers\"] \n",
    "plot_movie_with_two_features(\"batman returns\", training, \"water\", \"feel\")\n",
    "plots.axis([-0.001, 0.0011, -0.004, 0.008]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do movies in the training set relate to Batman Returns, using \"money\" and \"feel\"?\n",
    "plot_movie_with_two_features(\"batman returns\", train_movies, \"money\", \"feel\")\n",
    "plots.axis([-0.001, 0.0015, -0.001, 0.006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With \"friend\" and \"power\"?\n",
    "plot_movie_with_two_features(\"batman returns\", training, \"friend\", \"power\")\n",
    "plots.axis([-0.001, 0.0015, -0.001, 0.006])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 7:** First, write a function to compute the Euclidean distance between two **arrays** of features of *arbitrary* (but equal) length. With our dataset, each item in the array will correspond to the proportion of times a specific word was said.\n",
    "\n",
    "**Note:** For `n` different features, we compute the difference between corresponding feature values for two movies, square each of the `n`  differences, sum up the resulting numbers, and take the square root of the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(features0, features1):\n",
    "    \"\"\"Compute the Euclidean distance between two arrays of feature values. \n",
    "    Each features array should correspond to the values of a specific movie.\"\"\"\n",
    "    ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(make_array(0, 0), make_array(3, 4))\n",
    "### According to the Pythagorean Theorem (a^2 + b^2 = c^2), this line should return 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 8:** Now, write another function that finds the distance between two movies and two specific features. In our case, we will assume that the lower the distance between two movies is, the more similar they are. We can use that assumption to help classify movies later on.\n",
    "\n",
    "**Note:** Remember that if you have a row data type, you can use `item` to get a value from a column by its name. For example, if `r` is a row, then `r.item(\"Genre\")` is the value in column `\"Genre\"` in row `r`. You should also use the `row_for_title` function from above as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_features_distance(title0, title1, x_feature, y_feature):\n",
    "    \"\"\"Compute the distance between two movies with titles title0 and title1 as strings, and only use\n",
    "    the features x_feature and y_feature to calculate the distance.\"\"\"\n",
    "    row0 = ...\n",
    "    row1 = ...\n",
    "    ...\n",
    "    \n",
    "## Here is the difference between two movies.\n",
    "for movie in make_array(\"the terminator\", \"the avengers\"):\n",
    "    movie_distance = two_features_distance(movie, \"batman returns\", \"water\", \"feel\")\n",
    "    print(movie, 'distance:\\t', movie_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 9:** Now, what are the names and genres of the 5 movies in the **training set** closest to *The Terminator*? To answer this question, we will use the features \"water\" and \"feel\" and make a table called `close_movies` containing those 5 movies with columns `\"Title\"`, `\"Genre\"`, `\"money\"`, and `\"feel\"`, as well as a column called `\"distance from terminator\"` that contains the distance from *The Terminator.* The table should be sorted in ascending order by `distance from terminator`.\n",
    "\n",
    "*Hint:* We've provided you with the `distance_from_arnold` function, which might be helpful to answer this question. You don't need to use it, but it might help. You should use `train_movies.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_from_arnold(title):\n",
    "    return two_features_distance(\"the terminator\", title, \"money\", \"feel\")\n",
    "\n",
    "...\n",
    "\n",
    "close_movies = ...\n",
    "close_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 2:** Based on the info in the `close_movies` table, what genre would you classify \"The Terminator\" as? Why? How did you make that decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Choosing the Features\n",
    "\n",
    "Unfortunately, we usually cannot use all of the features in our classifier. One reason why is *computational* -- computing Euclidean distances takes a long time when we have many (5000!) features. Above, we only used two features; let's expand this.\n",
    "\n",
    "We're going to select a set of words that are very *discriminative*. That is, we hope these features will lead us to correctly classify as much of the test set as possible.  This process of choosing features that will make a classifier work well is sometimes called *feature selection*, or more broadly *feature engineering*.\n",
    "\n",
    "Now, choose your features. Select a set of common words (10-20 is usually enough) that you think might let you distinguish between *romance* and *action* movies. Make sure to choose words that are frequent enough that every movie contains at least one of them. A row from the movies table is shown below to help you choose your features.\n",
    "\n",
    "You can see how the words are distributed by genre in the plot below:\n",
    "![Word Plot](word_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set my_features to an array of features (strings that are column labels).\n",
    "\n",
    "my_features = ...\n",
    "\n",
    "## Do not change the code below.\n",
    "train_features = train_movies.select(my_features)\n",
    "test_features = test_movies.select(my_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 3:** Please explain your reasoning behind selecting these specific features. In general, what characteristics are you looking for when you choose features for a K-NN classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "manual_problem_id": "music_3_1_2"
   },
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Putting It All Together\n",
    "\n",
    "Now that we have chosen an algorithm and specific features to look at, we need to apply it to the data. Remember: a K-nearest neighbors classifier takes an unknown data point, checks the classes of the nearby data points, and classifies the unknown point as the majority class out of K neighbors. For example, if an unknown movie is near 3 romance movies and 2 action movies in terms of Euclidean distance, we would classify that movie as romance.\n",
    "\n",
    "Therefore, we need a few more functions to complete our classifier, which will allow us to do the following steps:\n",
    "\n",
    "1) Given an unknown movie and a set of known movies with their classes, find the distance between the unknown movie and all of the known movies in the training set.\n",
    "\n",
    "2) Sort a table with the classes and distances, so that the movies with the closest distances are at the top of the table.\n",
    "\n",
    "3) Choose the most similar K rows (the ones that are closest in distance) and find the most common class in that group of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 10:** First, we need to write a function that will find the most common class in a column of a table. It will take in a table and a string containing the name of the column in the table we want to analyze. \n",
    "\n",
    "For example, if a table (tbl) has a column called \"Classes\", with the values [\"romance\", \"romance\", \"action\", \"action\", \"action\"], we should be able to type most_common(\"Classes\", tbl) and it will output \"action\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def most_common(label, table):\n",
    "    \"\"\"The most common element in a column of a table.\n",
    "    \n",
    "    This function takes two arguments:\n",
    "      label: The label of a column, a string.\n",
    "      table: A table.\n",
    "     \n",
    "    It returns the most common value in that column of that table.\n",
    "    In case of a tie, it returns any one of the most common values\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code should output \"romance\".\n",
    "class_example = Table().with_column(\"Classes\", [\"romance\", \"romance\", \"action\"])\n",
    "most_common(\"Classes\", class_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are using Euclidean distance to determine similarity between movies.  When we run our classifier, we will need to calculate the Euclidean distances from an unknown movie (using the selected features) to all movies in the training set.  You could do this with a `for` loop and the distances function you wrote previously, but to make it computationally faster, we have provided a function, `fast_distances`, to do this for you.  Read its documentation to make sure you understand what it does.  (You don't need to read the code in its body unless you want to.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell to define fast_distances.\n",
    "def fast_distances(test_row, train_rows):\n",
    "    \"\"\"An array of the distances between test_row and each row in train_rows.\n",
    "\n",
    "    Takes 2 arguments:\n",
    "      test_row: A row of a table containing features of one\n",
    "        test movie (e.g., test_features.row(0)).\n",
    "      train_rows: A table of features (for example, the whole\n",
    "        table train_features).\"\"\"\n",
    "    assert train_rows.num_columns < 50, \"Make sure you're not using all the features of the movies table.\"\n",
    "    counts_matrix = np.asmatrix(train_rows.columns).transpose()\n",
    "    diff = np.tile(np.array(test_row), [counts_matrix.shape[0], 1]) - counts_matrix\n",
    "    np.random.seed(0) # For tie breaking purposes\n",
    "    distances = np.squeeze(np.asarray(np.sqrt(np.square(diff).sum(1))))\n",
    "    eps = np.random.uniform(size=distances.shape)*1e-10 #Noise for tie break\n",
    "    distances = distances + eps\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to be able to classify an unknown movie according to the steps explained above. \n",
    "\n",
    "**Coding Question 11:** Write a function called `classify`.  It should take the following four arguments in the same order:\n",
    "* A row of features for a movie to classify (e.g., `test_features.row(0)`).\n",
    "* A table with a column for each feature (e.g., `train_features`).\n",
    "* An array of classes that has as many items as the previous table has rows, and in the same order.\n",
    "* `k`, the number of neighbors to use in classification.\n",
    "\n",
    "It should return the class a `k`-nearest neighbor classifier picks for the given row of features (the string `'Romance'` or the string `'Action'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_row, train_table, train_labels, k):\n",
    "    \"\"\"Return the most common class among k nearest neigbors to test_row.\"\"\"\n",
    "    distances = fast_distances(test_row, train_table)\n",
    "    genre_and_distances = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The following cell should output \"action\". \n",
    "king_kong = movies.where(\"Title\", \"king kong\").select(my_features).row(0)\n",
    "classify(king_kong, train_features, train_movies.column(\"Genre\"), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 4:** In the cell above, we try to classify a movie from our training set with K = 1. What will the accuracy of our classifier be if we only classified movies from the **training set** with K = 1? Why?\n",
    "\n",
    "Now, imagine we used different values of K. What is the issue with using an even value for K? If, for example, we had 75 action movies and 25 romance movies in our training set, what would be the issue with using K = 51? In that situation, what would our accuracy be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 12:** Using your `classify` function, write a new function that takes as its argument a row containing the features of an unknown movie and a k-value for classification and classifies that row using the k-nearest neighbors algorithm with `train_features` as its training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_one_argument(unknown_row, k):\n",
    "    ...\n",
    "    \n",
    "# When you're done, this should produce 'Romance' or 'Action'.\n",
    "classify_one_argument(test_features.row(0), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5) Evaluating your classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's easy to use the classifier, let's see how accurate it is on the whole test set.\n",
    "\n",
    "**Coding Question 13:** Use `classify_one_argument` and a for loop to classify every movie in the test set. Choose an appropriate k-value. Name these guesses `test_guesses`.  **Then**, compute the proportion of correct classifications and set that value to `proportion_correct.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_guesses = ...\n",
    "for ... in ...:\n",
    "    ...\n",
    "    \n",
    "proportion_correct = ...\n",
    "proportion_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 5:** After creating your first classifier, please write your accuracy as a percentage below. Then, try changing some parts of your classifier (the features, K, etc.) by adjusting previous coding cells to try to increase the accuracy. What is the new accuracy? What did you change? Why did this affect your classifier? \n",
    "\n",
    "*Note:* In reality, you would use a separate dataset- known as a validation set- to test your classifier. It is a very bad idea to \"re-train\" your dataset on the test set, since it runs the risk of potentially biasing your classifier to the test set (which may or may not truly represent the population). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you've gone through one cycle of classifier design.  Let's summarize the steps:\n",
    "1. From available data, select test and training sets.\n",
    "2. Choose an algorithm you're going to use for classification.\n",
    "3. Identify some features.\n",
    "4. Define a classifier function using your features and the training set.\n",
    "5. Evaluate its performance (the proportion of correct classifications) using the test set.\n",
    "\n",
    "Keep in mind that this is a very basic form of supervised machine learning. If you continue in data science and take courses such as Computer Science 289: Intro to Machine Learning or Data Science 200: Principles and Techniques of Data Science, you will learn ways to improve this classifier and other ML techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2. Using Real World Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of assignments in this class have dealt with handpicked data that do not represent the real world at all; most of the time, our data is not clean. Here, we are going to expose you to another dataset.\n",
    "\n",
    "This data comes from North Fair Oaks, a census-designated place in San Mateo County. The area is also referred to as \"Little Michoacán\" due to the large amount of residents of Mexican descent.\n",
    "\n",
    "The dataset in question is a record of Violations to the Enforcment Code of North Fair Oaks. [Taken from their website:](https://planning.smcgov.org/documents/north-fair-oaks-code-enforcement-guide)\n",
    "\n",
    "```\n",
    "North Fair Oaks Code Enforcement Guide\n",
    "It is an effort being coordinated by the County of San Mateo to improve the condition of property in the North Fair Oaks area. If you live or do business in North Fair Oaks, you probably share the County's concern about the accumulation of abandoned vehicles, junk, trash and weeds on some properties in the community.\n",
    "```\n",
    "\n",
    "We have cleaned the majority of the dataset for you. The original dataset contained a description of the violation, where we have transformed it into proportions of the top 25 words accross all descriptions. We also have a class and latitude and longitude points for each complaint. If you are interested in how to do, the process is called Natural Language Processing, and in Python, the main tool is NLTK - Natural Language Toolkit. You can find more information [here.](https://www.nltk.org/)\n",
    "\n",
    "Each row represents a violation tied to a single property. For example, if a house has junk, weeds, grafitti, or various other forms of disrepair, it is labeled as `Disrepair`. If there are additional people living on the property, or an illegal addition to a house has been made it is labeled, `Addition`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations_original = Table.read_table(\"violations.csv\")\n",
    "violations_original.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1) Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most part, our dataset is clean. However, there are a few things we need to remove before we can continue our analysis. \n",
    "\n",
    "First, the `Unnamed: 0` column is unnecessary; it only exists to label the index of each row. Second, we want to focus on the SF Bay Area (particularly North Fair Oaks) and the dataset includes a few extra data points outside of that region. \n",
    "\n",
    "**Coding Question 14:** Clean the dataset, called `violations_original`, by removing the first column and removing any rows that do NOT have a latitude between 37.4 and 37.5 and a longitude between -122.3 and -122. Call the new table `violations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations = ...\n",
    "violations.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2) Splitting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the training and test sets, using the same process we used to split the movies dataset. Set training proportion to the proportion of the dataset you want to use to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the training proportion below.\n",
    "training_proportion = ...\n",
    "\n",
    "### Do not change the code below.\n",
    "violations_permuted = violations.sample(with_replacement = False)\n",
    "num_violations = violations.num_rows\n",
    "num_train = int(training_proportion * num_violations)\n",
    "train_violations = violations_permuted.take(np.arange(num_train))\n",
    "num_test = num_violations - num_train\n",
    "num_addition = train_violations.where(\"CLASS\", \"Addition\").num_rows\n",
    "num_disrepair = train_violations.where(\"CLASS\", \"Disrepair\").num_rows\n",
    "test_violations = violations_permuted.take(np.arange(num_train, num_violations))\n",
    "print(\"The number of reports in the training set is \" + str(num_train) + \".\",\n",
    "      \"The number of reports in the test set is \" + str(num_test) + \".\")\n",
    "print(\"In the training set, there are \" + str(num_addition) + \" addition reports.\",\n",
    "     \"There are \" + str(num_disrepair) + \" disrepair reports.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3) Judging Quality of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells will explore how the data looks for 50 random reports. Like with movies, we will use two words, one on the X and one on the Y, so we have a two-dimensional plot. The colors of the points clarify the different types of reports: Additions and Disrepair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do not change the following cell.\n",
    "def plot_with_two_features(training_records, x_feature, y_feature):\n",
    "    \n",
    "    plots = Table().with_columns(\n",
    "        x_feature, [training_records.row(0).item(x_feature)],\n",
    "        y_feature, [training_records.row(0).item(y_feature)],\n",
    "        \"Color\", [\"unknown\"],\n",
    "    )\n",
    "\n",
    "    for i in np.arange(training_records.num_rows):\n",
    "        row = training_records.row(i)\n",
    "        plots.append([row.item(x_feature), row.item(y_feature), row.item(\"CLASS\")])\n",
    "\n",
    "    plots.scatter(x_feature, y_feature, colors=\"Color\", s=30)\n",
    "\n",
    "training_examples = train_violations.take(np.arange(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_two_features(train_violations, \"graffiti\", \"illegal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_with_two_features(train_violations, \"dumping\", \"illegal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 6:** Comment on the plots above. What do you notice? Is this different from the movies dataset and if so, how? Judging from these specific words (and any additional graphs you generate, if you are interested in exploring more), would these features be better or worse for classification than the movie features? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feel free to check any other words.\n",
    "plot_with_two_features(train_violations, ..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Mapping It!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we will practice mapping geographic data. Remember the maps function built into datascience - you can find more information in the datascience documentation [here.](http://data8.org/datascience/maps.html) \n",
    "\n",
    "The next cell creates the tables and functions necessary for maps. In this case, we want to color Additions `black` and Disrepairs `yellow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do not change the code in this cell.\n",
    "graphed_violations = violations.sample(50)\n",
    "def color_func(x):\n",
    "    if x == \"Addition\":\n",
    "        return 'black'\n",
    "    else:\n",
    "        return 'yellow'\n",
    "    \n",
    "color_array = make_array()\n",
    "for i in graphed_violations.column(\"CLASS\"):\n",
    "    val = color_func(i)\n",
    "    color_array = np.append(color_array, val)\n",
    "    \n",
    "to_map = Table().with_columns(\n",
    "    \"lat\", graphed_violations.column(\"lat\"),\n",
    "    \"long\", graphed_violations.column(\"long\"),\n",
    "    \"label\", graphed_violations.column(\"CLASS\"),\n",
    "    \"color\", color_array\n",
    "    )\n",
    "to_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell plots the 50 randomly sampled reports, called `map_table` below, by using the maps function. For formatting reasons, the only argument we will need to input is map_table, since the table columns are already in the correct order. A large red box may appear when you generate the map; this should not be an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell. If you don't see any points, zoom out and go to the Bay Area in California.\n",
    "Marker.map_table(to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 7:** What do you notice about the map above? What are some potential causes for your observations? \n",
    "\n",
    "In this case, do you think would geographic data be useful as features in a classifier? If so, how would you use that data to create said classifier? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Writing the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the bag-of-words classifier again. We will ignore the latitude and longitude data, since it will require some extra code/unit conversions that is out of the scope of this course. If you are interested in using geographic data for classification, we recommend looking into GIS (Geographic Information System). That being said, remember the process with movies: \n",
    "\n",
    "1) Given an unknown data point and a set with known classes (the training set), find the distance between the unknown point and all of the known points in the training set.\n",
    "\n",
    "2) Attach the distances to a table with the classes, and sort the table so that the points with the smallest distances are at the top of the table.\n",
    "\n",
    "3) Choose the most similar K rows (the ones that are closest in distance) and find the most common class in that group of K.\n",
    "\n",
    "To do so, we will first need to choose our features. Assign the `features` to a list or array of features (words) you want to use in your classification. One row of the violations table, containing the features, is shown below for your reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violations.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select your features here.\n",
    "features = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 15:** Now that you have your features selected, write a function called `classify_violation` that will classify a report for you. Feel free to use any functions we have used or written earlier in this project. If you are struggling, try reviewing the movies or songs classifiers. \n",
    "\n",
    "It should take 2 inputs: `row` being a specific report you would like to classify and `k` being the number of nearest neighbors you would like to use in your classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_violation(row, k):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, classify 2 training violations with a K value of 1 to test if our classifier works. The report with the ID `VIO2016-00285` should output `Addition`, while the report with the ID of `VIO2017-00430` should output `Disrepair`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIO2016_00285 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIO2016_00430 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Question 16:** Now that we know that our function works, let's try to calculate the accuracy of the classifier. Use your classifier on the test set and find the accuracy as a proportion, setting that to `classifier_accuracy.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "classifier_accuracy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FRQ 8:** Depending on your features chosen, you may notice that the prediction accuracy is very high (>90%. If you are not getting this, try using different features). Why is this the case? \n",
    "\n",
    "What factors did you take into account when building your classifier? Knowing the number of `Disrepair` and `Addition` reports in the training set, what is the minimum value of K that would always result in every prediction being \"Disrepair\" or \"Addition\" only? \n",
    "\n",
    "Lastly, do you see any issues with using this classifier or dataset? If you could continue exploring this data, what would you like to research?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done!\n",
    "\n",
    "Congratulations! You wrote multiple K-nearest neighbors classifiers, classifying unknown movies and violation reports. If you're interested in learning more about classification, we recommend reading the [Data 200 textbook](https://www.textbook.ds100.org/ch/17/classification_intro.html). For example, you may ask: how do we choose the best K value? We can use cross-validation, explained here: https://www.textbook.ds100.org/ch/15/bias_cv.html. Classification is one of many machine learning tasks, and there are plenty of other classification algorithms! If you feel so inclined, we encourage you to try any methods you feel might help improve your classifier. \n",
    "\n",
    "We've compiled a list of blog posts with some more information about classification and machine learning. Create as many cells as you'd like below--you can use them to import new modules or implement new algorithms. \n",
    "\n",
    "Blog posts: \n",
    "\n",
    "* [Classification algorithms/methods](https://medium.com/@sifium/machine-learning-types-of-classification-9497bd4f2e14)\n",
    "* [Train/test split and cross-validation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n",
    "* [More information about k-nearest neighbors](https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7)\n",
    "* [Overfitting](https://elitedatascience.com/overfitting-in-machine-learning)\n",
    "\n",
    "In future data science classes, such as Data Science 200, you'll learn about some about some of the algorithms in the blog posts above, including logistic regression. You'll also learn more about overfitting, cross-validation, and approaches to different kinds of machine learning problems.\n",
    "\n",
    "There's a lot to think about, so we encourage you to find more information on your own!\n",
    "\n",
    "Modules to think about using:\n",
    "\n",
    "* [Scikit-learn tutorial](http://scikit-learn.org/stable/tutorial/basic/tutorial.html)\n",
    "* [TensorFlow information](https://www.tensorflow.org/tutorials/)\n",
    "\n",
    "...and many more!\n",
    "\n",
    "In the cell below, try to implement a new algorithm or module to classify the movies (OPTIONAL).\n",
    "\n",
    "Please submit this notebook to Gradescope by saving it as an .ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
