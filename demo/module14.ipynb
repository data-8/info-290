{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import * # from this library, import all functions\n",
    "from math import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg', warn=False)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding review: pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the data\n",
    "ces = pd.read_csv(\"calenviroscreen.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1: Inspect the data\n",
    "# head, tail, describe, shape\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracts with a pollution score of 75-100 represent \"disadvantaged communities\" under SB-535\n",
    "# Higher scores = greater pollution burden, population vulnerability\n",
    "# Q2: Visualize the data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Using a for loop, create an array to label all disadvantaged communities \n",
    "# If the score is between 75-100, give it the label \"At Risk\"\n",
    "# Otherwise, \"Not at Risk\"\n",
    "scores = ces.ces_pollution_score # access the data as a series\n",
    "\n",
    "...\n",
    "\n",
    "for ...:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q4: Add the labels back onto the ces table as the column name \"class\"\n",
    "# Try using bracket notation \n",
    "# or using df.insert(loc = ..., column = ..., value = ...)\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5: how many census tracts are at risk? \n",
    "# use df.groupby(\"Group\")[\"Column\"].func()\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: although this is informative, it's not great for a K-NN classifier \n",
    "# because we usually want decent representation of all classes.\n",
    "# For purposes of today's exercise, let's say that \"At risk\" refers to a score of\n",
    "# 50-100 just so we have good representation of both groups\n",
    "\n",
    "ces = pd.read_csv(\"calenviroscreen.csv\")\n",
    "\n",
    "def add_labels(df):\n",
    "    scores = df[\"ces_pollution_score\"]\n",
    "    labels = [\"At risk\" if 50 <= x <= 100 else \"Not at risk\" for x in scores] # this is a list comprehension\n",
    "    df.insert(loc = 0, column = \"Class\", value = labels)\n",
    "    return df\n",
    "\n",
    "new_ces = add_labels(ces)\n",
    "new_ces.groupby(\"Class\")[\"census_tract\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Much better! Q6: let's export this to work with in datascience\n",
    "# remember to remove the indices!\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ces_labeled = Table().read_table(\"ces_labeled.csv\").drop(\"ces_pollution_score\") # dropping the aggregate\n",
    "ces_labeled.show(5)\n",
    "ces_labeled.num_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It looks like our data is ordered by pollution score. We want to make sure\n",
    "# we get representative samples for both the test set and training set \n",
    "# so they both properly represent the population.\n",
    "# Q1: Permute the dataset and then create a separate training and test set by an 80/20 ratio. \n",
    "num_train_rows = int(ces_labeled.num_rows * 0.8)\n",
    "num_test_rows = int(ces_labeled.num_rows - num_train_rows)\n",
    "\n",
    "ces_shuffled = ...\n",
    "ces_train = ...\n",
    "ces_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check if the ratios are relatively the same\n",
    "grouped_train = ces_train.group(0)\n",
    "grouped_train.with_column(\"prop\", grouped_train.column(\"count\") / sum(grouped_train.column(\"count\"))).show()\n",
    "grouped_test = ces_test.group(0)\n",
    "grouped_test.with_column(\"prop\", grouped_test.column(\"count\") / sum(grouped_test.column(\"count\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Choosing the features/attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The list of our attributes are below:\n",
    "ces_train.labels[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In general, we want traits that differentiate the 2 groups relatively well\n",
    "def create_scatter_group(attrib1, attrib2):\n",
    "    ces_train.sample(250).scatter(attrib1, attrib2, group = 0)\n",
    "\n",
    "create_scatter_group(\"poverty\", \"ozone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another fun thing - interactive widgets!\n",
    "# Choose traits you want to compare\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "interact(create_scatter_group, attrib1 = list(ces_train.labels[2:]), attrib2 = list(ces_train.labels[2:]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One issue you may have noticed is the wide range of values for all of the variables used\n",
    "# This isn't good! In K-NN, we want to make sure we aren't weighting one attribute higher than another.\n",
    "# Let's fix this by converting to standard units. \n",
    "# Q: Complete the function:\n",
    "\n",
    "def standard_units(arr):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will use your std units function to normalize all quant. vars.\n",
    "def norm_tbl(tbl):\n",
    "    new_tbl = tbl.select(0, 1, 2)\n",
    "    for col in tbl.labels[2:]:\n",
    "        converted = standard_units(tbl.column(col))\n",
    "        new_tbl = new_tbl.with_column(col, converted)\n",
    "    return new_tbl\n",
    "\n",
    "norm_train = norm_tbl(ces_train)\n",
    "norm_test = norm_tbl(ces_test)\n",
    "\n",
    "norm_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the data looks relatively the same, but now on different scales\n",
    "norm_train.sample(200).scatter(\"poverty\", \"ozone\", group = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think that it makes the most sense to use the environmental/population features instead of demographic\n",
    "# (although it looks like race is correlated with class)\n",
    "our_attributes = list(ces_train.labels[9:])\n",
    "our_attributes\n",
    "\n",
    "train_atts = ces_train.select(our_attributes)\n",
    "test_atts = ces_test.select(our_attributes)\n",
    "train_atts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculating distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, we would take the Euclidean distance between your unknown and every row in the training set\n",
    "# i.e. np.sqrt((x1 - x2)**2 + (y1 - y2)**2 +(z1 - z2)**2)\n",
    "# but that's very computationally expensive.\n",
    "# We've written a function for you below, which uses row objects\n",
    "test_atts.row(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_distances(test_row, train_table):\n",
    "    \"\"\"Return an array of the distances between test_row and each row in train_rows.\n",
    "\n",
    "    Takes 2 arguments:\n",
    "      test_row: A row of a table containing features of one\n",
    "        test movie (e.g., test_my_features.row(0)).\n",
    "      train_table: A table of features (for example, the whole\n",
    "        table train_my_features).\"\"\"\n",
    "    assert train_table.num_columns < 50, \"Make sure you're not using all the features of the movies table.\"\n",
    "    counts_matrix = np.asmatrix(train_table.columns).transpose()\n",
    "    diff = np.tile(np.array(list(test_row)), [counts_matrix.shape[0], 1]) - counts_matrix\n",
    "    np.random.seed(0) # For tie breaking purposes\n",
    "    distances = np.squeeze(np.asarray(np.sqrt(np.square(diff).sum(1))))\n",
    "    eps = np.random.uniform(size=distances.shape)*1e-10 #Noise for tie break\n",
    "    distances = distances + eps\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "practice_unknown = test_atts.row(0)\n",
    "print(\"The class of this row in the test set is \" + ces_test.row(0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can take that array of distances and add it back onto the training table\n",
    "# and then sort to find the \"nearest\" neighbors\n",
    "# Q: Find the distances between the practice and all rows in the training set (train_atts) \n",
    "# Then, add those distances to the table with the labels: ces_train \n",
    "practice_distances = ...\n",
    "train_with_dist = \n",
    "train_with_dist.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: classifying individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have the table with distances, we can classify an individual \n",
    "# by taking a majority vote from the K-nearest neighbors\n",
    "# let's say k = 15\n",
    "\n",
    "# Q: Classify the practice row using k=15. What is the \"majority\" vote?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: testing the accuracy of our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a general idea of the accuracy of the classifier, we can do Steps 3-4 for all rows in the test set\n",
    "# And seeing how many are correct.\n",
    "\n",
    "def classify_one(row, k=15):\n",
    "    distance_from_row = fast_distances(row, train_atts)\n",
    "    with_dist = ces_train.with_column(\"Distance\", distance_from_row).sort(\"Distance\", descending = False)\n",
    "    return with_dist.take(np.arange(k)).group(0).sort(1, descending = True).column(0).item(0)\n",
    "\n",
    "classify_one(test_atts.row(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_guesses = test_atts.apply(classify_one)\n",
    "actual_classes = ces_test.column(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q: Calculate the proportion of \"correct\" guesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this more accurate than just blindly guessing \"Not at risk\"?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
